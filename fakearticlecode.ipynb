{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"19h4EcPUVgurkIZARoHzgDrMKzA9GWVyT","authorship_tag":"ABX9TyN2eo8MLPKTvTVd0+DKPtUw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"HNsuuz3eIwAD","executionInfo":{"status":"error","timestamp":1716378680590,"user_tz":-300,"elapsed":5,"user":{"displayName":"hamza latif","userId":"08159994876973782413"}},"outputId":"8b98a7ae-18eb-4597-d5c5-7d4100f3bb6f"},"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-3-268b99e807c6>, line 26)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-268b99e807c6>\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    Step 2: Text Preprocessing\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["\n","#Step 1: Create a Dummy Dataset\n","\n","import numpy as np\n","import pandas as pd\n","\n","# Generate dummy dataset\n","np.random.seed(42)\n","\n","# Genuine reviews\n","genuine_reviews = [\"The product is excellent\", \"I love this product\", \"Highly recommend\", \"Very satisfied\", \"Will buy again\"] * 100\n","\n","# Fake reviews (anomalous)\n","fake_reviews = [\"Buy this product now\", \"This is a scam\", \"Don't waste your money\", \"Worst product ever\", \"Fake review\"] * 20\n","\n","# Create a DataFrame\n","reviews = pd.DataFrame({\n","    'review': genuine_reviews + fake_reviews,\n","    'label': [0] * len(genuine_reviews) + [1] * len(fake_reviews)  # 0: genuine, 1: fake\n","})\n","\n","# Shuffle the dataset\n","reviews = reviews.sample(frac=1).reset_index(drop=True)\n","print(reviews.head())\n","\n","\n","Step 2: Text Preprocessing\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Create TF-IDF vectorizer\n","vectorizer = TfidfVectorizer(max_features=100)  # Limit to 100 features for simplicity\n","\n","# Fit and transform the reviews\n","X = vectorizer.fit_transform(reviews['review']).toarray()\n","print(X.shape)\n","\n","#Step 3: Build and Train the Autoencoder\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense\n","\n","# Define the autoencoder\n","input_dim = X.shape[1]\n","encoding_dim = 32\n","\n","input_layer = Input(shape=(input_dim,))\n","encoded = Dense(encoding_dim, activation='relu')(input_layer)\n","decoded = Dense(input_dim, activation='sigmoid')(encoded)\n","\n","autoencoder = Model(inputs=input_layer, outputs=decoded)\n","autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n","\n","# Train the autoencoder on genuine reviews only\n","genuine_X = X[reviews['label'] == 0]\n","autoencoder.fit(genuine_X, genuine_X, epochs=50, batch_size=16, shuffle=True)\n","\n","Step 4: Detect Fake Reviews\n","# Compute reconstruction error for all reviews\n","reconstructions = autoencoder.predict(X)\n","mse = np.mean(np.power(X - reconstructions, 2), axis=1)\n","\n","# Set a threshold for the reconstruction error\n","threshold = np.percentile(mse, 95)  # 95th percentile of the reconstruction error on genuine reviews\n","\n","# Predict fake reviews\n","predicted_labels = (mse > threshold).astype(int)\n","\n","# Compare with actual labels\n","results = pd.DataFrame({'review': reviews['review'], 'actual': reviews['label'], 'predicted': predicted_labels, 'mse': mse})\n","print(results.head(20))"]}]}